{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `chatXiv` - Chat with arXiv papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bhavesh/miniconda3/envs/llm/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import arxiv\n",
    "import requests\n",
    "import fitz\n",
    "from pymupdf_rag import to_markdown\n",
    "from typing import List, Tuple\n",
    "\n",
    "import chromadb\n",
    "from llama_index.core import Document, VectorStoreIndex, PromptTemplate\n",
    "from llama_index.core.schema import NodeWithScore\n",
    "from llama_index.core.storage.docstore import SimpleDocumentStore\n",
    "from llama_index.vector_stores.chroma import ChromaVectorStore\n",
    "from llama_index.core.ingestion import IngestionPipeline\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "from llama_index.core.node_parser import MarkdownNodeParser\n",
    "import chromadb.utils.embedding_functions as embedding_functions\n",
    "from chat_utils import ChatLLM, ChatSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['OPENAI_API_KEY'] = \"YOUR API KEY\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Q&A over a PDF Document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ingestion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "arxiv_client = arxiv.Client()\n",
    "\n",
    "def download_pdf(paper_id: str, dirpath: str = './papers'):\n",
    "    '''Downloads a paper from arXiv given its ID'''\n",
    "    paper = next(arxiv.Client().results(arxiv.Search(id_list=[paper_id])))\n",
    "    return paper.download_pdf(dirpath=dirpath), paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_arxiv_document(paper_id):\n",
    "    '''Loads an arxiv paper as a Document object'''\n",
    "    pdf_path, paper = download_pdf(paper_id, dirpath='./papers')\n",
    "    paper_pdf = fitz.open(pdf_path)\n",
    "    md_text = to_markdown(paper_pdf)\n",
    "    \n",
    "    return Document(\n",
    "        doc_id=paper_id,\n",
    "        text=md_text,\n",
    "        metadata={\n",
    "            'title': paper.title,\n",
    "            'authors': \", \".join([auth.name for auth in paper.authors[:10]]),\n",
    "            'published': paper.published.strftime('%Y-%m-%d'),\n",
    "            'filepath': pdf_path,\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "paper_id = \"2312.00752\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the documents\n",
    "documents = [\n",
    "    load_arxiv_document(paper_id)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Breakdown the document into nodes\n",
    "parser = MarkdownNodeParser()\n",
    "\n",
    "nodes = parser.get_nodes_from_documents(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the VectorDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a Chroma VectorDB instance\n",
    "db = chromadb.PersistentClient(path=\"./chroma_db\")\n",
    "openai_ef = embedding_functions.OpenAIEmbeddingFunction(\n",
    "                api_key=os.getenv(\"OPENAI_API_KEY\"),\n",
    "                model_name=\"text-embedding-3-small\"\n",
    "            )\n",
    "chroma_collection = db.get_or_create_collection(\"chatxiv\", embedding_function=openai_ef)\n",
    "\n",
    "# Create a Chroma VectorStore\n",
    "vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
    "\n",
    "# Create a Simple Document Store as well\n",
    "docstore = SimpleDocumentStore()\n",
    "\n",
    "# Load the embedding model\n",
    "embed_model = OpenAIEmbedding(model=\"text-embedding-3-small\", embed_batch_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing nodes: 100%|██████████| 1/1 [00:00<00:00, 18.62it/s]\n",
      "Generating embeddings: 100%|██████████| 43/43 [00:02<00:00, 16.46it/s]\n"
     ]
    }
   ],
   "source": [
    "# Build the Embedding Pipeline\n",
    "pipeline = IngestionPipeline(\n",
    "    transformations=[\n",
    "        parser,\n",
    "        embed_model,\n",
    "    ],\n",
    "    vector_store=vector_store,\n",
    "    docstore=docstore,\n",
    ")\n",
    "# load local cache (if it exists)\n",
    "# pipeline.load(\"./pipeline_chatxiv\")\n",
    "\n",
    "pipeline.run(documents=documents, show_progress=True)\n",
    "\n",
    "# save cache locally\n",
    "pipeline.persist(\"./pipeline_chatxiv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create vector index for retrieval\n",
    "index = VectorStoreIndex.from_vector_store(\n",
    "    vector_store=vector_store,\n",
    "    embed_model=embed_model\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = index.as_retriever(similarity_top_k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_str = \"Explain mamba\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[NodeWithScore(node=TextNode(id_='2129918f-3efe-4f77-bd90-2b7111fe6d6b', embedding=None, metadata={'Header_1': 'Mamba: Linear-Time Sequence Modeling with Selective State Spaces', 'Header_2': 'E ## Experimental Details and Additional Results', 'Header_3': 'E.5 ### Efciency Benchmark', 'title': 'Mamba: Linear-Time Sequence Modeling with Selective State Spaces', 'authors': 'Albert Gu, Tri Dao', 'published': '2023-12-01', 'filepath': './papers/2312.00752v1.Mamba__Linear_Time_Sequence_Modeling_with_Selective_State_Spaces.pdf'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='2312.00752', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'title': 'Mamba: Linear-Time Sequence Modeling with Selective State Spaces', 'authors': 'Albert Gu, Tri Dao', 'published': '2023-12-01', 'filepath': './papers/2312.00752v1.Mamba__Linear_Time_Sequence_Modeling_with_Selective_State_Spaces.pdf'}, hash='2d42815c6988e92465aaa9c4d2f6123a4c5315cf904163a619f54b63ed7fd8f7'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='6183f59a-0b97-407d-ad95-e379e3c013b7', node_type=<ObjectType.TEXT: '1'>, metadata={'Header_1': 'Mamba: Linear-Time Sequence Modeling with Selective State Spaces', 'Header_2': 'E ## Experimental Details and Additional Results', 'Header_3': 'E.4 ### Audio Details', 'title': 'Mamba: Linear-Time Sequence Modeling with Selective State Spaces', 'authors': 'Albert Gu, Tri Dao', 'published': '2023-12-01', 'filepath': './papers/2312.00752v1.Mamba__Linear_Time_Sequence_Modeling_with_Selective_State_Spaces.pdf'}, hash='84bdc62c6c53851f43e9f2c2515bb4f20d5f9c4a84d849fa968331844645e3e1')}, text='E.5 ### Efciency Benchmark\\n\\n**Scan Operation.** We compare the core operation of selective SSMs, which is the parallel scan (Section 3.3 ),\\nagainst convolution and attention, measured on an A100 80GB PCIe GPU. Note that these do not include the cost\\nof other operations outside of this core operation, such as computing the convolutional kernel in global-convolution\\nmodels, or computing the QKV projections in attention.\\n\\nAs a baseline, we implement a standard parallel scan in PyTorch with no kernel fusion. This requires materializing\\nthe parameters **_A_** , **_B_** , **_C_** in HBM.\\n\\nOur scan implementation fuses the discretization step and the parallel scan, avoiding the cost of materializing all\\nthe large parameters in HBM.\\n\\nFor convolution, we use the standard implementation in PyTorch, which separately performs FFTs on the inputs\\nand the ﬁlters, multiply them in frequency domain, then performs an inverse FFT to obtain the result. The\\ntheoretical complexity is 푂(퐿log(퐿)) for sequence length 퐿 .\\n\\nFor attention, we compare against the fastest implementation that we are aware of (FlashAttention-2 (Dao 2023 )),\\nwith causal mask. Note that FlashAttention-2 with causal mask is about 1.7 × faster than without causal mask,\\nsince approximately only half of the attention entries are computed.\\n\\nWe use batch size of 1 and increase the sequence length from 2 9  = 512 , 2 10  ≈1퐾 , 2 11  ≈2퐾 , up to 2 19  ≈500퐾\\n(some of the baselines run out of memory before reaching 500K). We use a model dimension of 퐷= 1024 and state\\ndimension 푁= 16 . We measure with BF16 inputs, which is the data type most commonly used for large scale\\ntraining.\\n\\n**End-to-end Inference.** We measure the inference throughput of a Mamba 1.4B model and an untrained Mamba\\n6.9B model, against a standard Transformer (GPT3 architecture) at 1.3B and 6.7B size. We use the standard\\nTransformer implementation in the Huggingface `transformers` library.\\n\\nWe set the prompt length to be 2048 and the generation length to be 128. We vary the batch size from 1, 2, 4, 8, 16,\\n32, 64, to 128, and measure time time taken to generate 128 tokens. We then calculate the throughput (tokens/s)\\nas batch size × 128∕ time taken . We repeat the measurements 3 times and take the average. Measurements are\\ndone on an A100 80GB PCIe GPU.\\n\\n**Memory Benchmark.** The memory usage simply scales proportionally to the size of the activation tensors, as\\nwith most deep sequence models. We report measurements of the training memory requirements of 125M models\\n\\n36\\n\\n\\n\\n-----\\n\\n\\nTable 15: ( **Memory benchmark** .) Mamba’s memory footprint is comparable to the most optimized Transformer. Results for 125M\\nmodels.\\n\\nBatch size Transformer (w/ FlashAttention-2) Mamba\\n\\n1 4.6GB 4.8GB\\n2 5.2GB 5.8GB\\n4 6.9GB 7.3GB\\n8 11.5GB 12.3GB\\n16 20.7GB 23.1GB\\n32 34.5GB 38.2GB\\n\\non 1 A100 80GB GPU. Each batch consists of sequences of length 2048. We compare to the most memory-eﬃcient\\nTransformer implementation we are aware of (with kernel fusion from `torch.compile` and with FlashAttention-2).\\nTable 15 shows that Mamba’s memory requirement is comparable to a similar-sized Transformer with an extremely\\noptimized implementation, and we expect further improvement in Mamba’s memory footprint in the future.\\n\\n37\\n\\n\\n\\n-----', start_char_idx=134657, end_char_idx=137915, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=0.335212814010058)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retrieved_nodes = retriever.retrieve(query_str)\n",
    "retrieved_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Header_1': 'Mamba: Linear-Time Sequence Modeling with Selective State Spaces',\n",
       " 'Header_2': 'E ## Experimental Details and Additional Results',\n",
       " 'Header_3': 'E.5 ### Efciency Benchmark',\n",
       " 'title': 'Mamba: Linear-Time Sequence Modeling with Selective State Spaces',\n",
       " 'authors': 'Albert Gu, Tri Dao',\n",
       " 'published': '2023-12-01',\n",
       " 'filepath': './papers/2312.00752v1.Mamba__Linear_Time_Sequence_Modeling_with_Selective_State_Spaces.pdf'}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retrieved_nodes[0].metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Response Synthesis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = {\n",
    "    'role': 'system',\n",
    "    'content': \"\"\"You are a Q&A bot. You are here to answer questions based on the context given.\n",
    "You are prohibited from using prior knowledge and you can only use the context given. If you need \n",
    "more information, please ask the user.\"\"\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_prompt = PromptTemplate(\n",
    "\"\"\"Context information to answer the query is below.\n",
    "---------------------\n",
    "{context_str}\n",
    "---------------------\"\"\")\n",
    "\n",
    "def build_context_prompt(retrieved_nodes):\n",
    "    context_str = \"\\n\\n\".join([r.node.get_content(metadata_mode='all') for r in retrieved_nodes])\n",
    "    return context_prompt.format(\n",
    "        context_str=context_str\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatLLM()\n",
    "session = ChatSession(llm, system_prompt=system_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the context as a user message to the message history (thread)\n",
    "session.thread.append({\n",
    "    'role': 'user',\n",
    "    'content': build_context_prompt(retrieved_nodes)\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mamba is like a super smart computer friend that can help with big puzzles really fast! It can look at lots of pieces of information at once and figure out the best way to put them together. It's very good at organizing things quickly and efficiently to solve problems.\n"
     ]
    }
   ],
   "source": [
    "response = session.chat(\"How does mamba work? Explain like I am 5\")\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieval as a Tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.tools import FunctionTool\n",
    "from openai.types.chat.chat_completion_tool_message_param import ChatCompletionToolMessageParam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def context_retrieval(search_query: str, top_k: int=3) -> Tuple[str, List[NodeWithScore]]:\n",
    "    '''\n",
    "    This function let's you semantically retrieve relevant context chunks from a given document based on a query.\n",
    "\n",
    "    Arguments:\n",
    "        query (str): The query to search for in the document. Based on the original user query, write a good search query\n",
    "                     which is more logically sound to retrieve the relevant information from the document.\n",
    "        top_k (int): The number of top chunks to retrieve from the document. Default is 3. You can increase this number if\n",
    "                     you feel like you need more information. But ideally you should make multiple calls to retrieve different\n",
    "                     topics of information.\n",
    "\n",
    "    Returns:\n",
    "        str: The top retrieved\n",
    "        List[NodeWithScore]: A list of nodes with their scores. Use this to cite the information in the response.\n",
    "    '''\n",
    "    retriever = index.as_retriever(similarity_top_k=top_k)\n",
    "    retrieved_nodes = retriever.retrieve(search_query)\n",
    "    return build_context_prompt(retrieved_nodes), retrieved_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatLLM()\n",
    "session = ChatSession(llm, system_prompt=system_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_retrieval_tool = FunctionTool.from_defaults(fn=context_retrieval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "available_functions = {\n",
    "    \"context_retrieval\": context_retrieval,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conversation_turn(user_query):\n",
    "    # Send the messages and available functions to the model\n",
    "    # We will call our chat method from our session object which is already keeping track of the history\n",
    "    response = session.chat(user_query, \n",
    "                            model='gpt-3.5-turbo', \n",
    "                            temperature=1,\n",
    "                            max_tokens=512,\n",
    "                            tools=[context_retrieval_tool.metadata.to_openai_tool()], tool_choice='auto')\n",
    "    \n",
    "    tool_calls = response.tool_calls\n",
    "\n",
    "    # Check if the model wanted to call a function\n",
    "    if tool_calls:\n",
    "        # Call the function\n",
    "        for tool_call in tool_calls:\n",
    "            function_name = tool_call.function.name\n",
    "            function_to_call = available_functions[function_name]\n",
    "            function_args = json.loads(tool_call.function.arguments)\n",
    "            function_response, retrieved_nodes = function_to_call(**function_args)\n",
    "\n",
    "            print(f'Retrieving with {tool_call}')\n",
    "\n",
    "            # Add the function response to the message thread\n",
    "            session.thread.append(\n",
    "                ChatCompletionToolMessageParam(role='tool', tool_call_id=tool_call.id, name=function_name, content=function_response)\n",
    "            )\n",
    "        \n",
    "        # Get a new response from the model where it can see the function response\n",
    "        post_function_response = session.chat()\n",
    "\n",
    "        return post_function_response\n",
    "    \n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving with ChatCompletionMessageToolCall(id='call_GgdsYoPcpBhFtZWios8xkRKI', function=Function(arguments='{\"search_query\":\"Mamba: Linear-Time Sequence Modeling with Selective State Spaces\",\"top_k\":1}', name='context_retrieval'), type='function')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Mamba works by utilizing a Selective State Space Model with hardware-aware state expansion. This approach involves incorporating a selection mechanism into the state space model to dynamically choose states or components based on specific criteria. Additionally, the model is designed to expand its state space efficiently by leveraging hardware-aware techniques, allowing for increased computational efficiency and performance.\\n\\nBy combining the Selective State Space Model with hardware-aware state expansion, Mamba aims to enable linear-time sequence modeling with improved adaptability and efficiency in processing sequential data. This approach integrates selective attention and hardware optimization to enhance the model's capabilities in capturing temporal dependencies and patterns in sequences.\""
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = conversation_turn(\"HOw does mamba work?\")\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
