{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `chatXiv` - Chat with arXiv papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bhavesh/miniconda3/envs/llm/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import arxiv\n",
    "import requests\n",
    "import fitz\n",
    "from pymupdf_rag import to_markdown\n",
    "from typing import List, Tuple\n",
    "\n",
    "import chromadb\n",
    "from llama_index.core import Document, VectorStoreIndex, PromptTemplate\n",
    "from llama_index.core.schema import NodeWithScore\n",
    "from llama_index.core.storage.docstore import SimpleDocumentStore\n",
    "from llama_index.vector_stores.chroma import ChromaVectorStore\n",
    "from llama_index.core.ingestion import IngestionPipeline\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "from llama_index.core.node_parser import MarkdownNodeParser\n",
    "import chromadb.utils.embedding_functions as embedding_functions\n",
    "from chat_utils import ChatLLM, ChatSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['OPENAI_API_KEY'] = \"YOUR API KEY\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Q&A over a PDF Document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ingestion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "arxiv_client = arxiv.Client()\n",
    "\n",
    "def download_pdf(paper_id: str, dirpath: str = './papers'):\n",
    "    '''Downloads a paper from arXiv given its ID'''\n",
    "    paper = next(arxiv.Client().results(arxiv.Search(id_list=[paper_id])))\n",
    "    return paper.download_pdf(dirpath=dirpath), paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_arxiv_document(paper_id):\n",
    "    '''Loads an arxiv paper as a Document object'''\n",
    "    pdf_path, paper = download_pdf(paper_id, dirpath='./papers')\n",
    "    paper_pdf = fitz.open(pdf_path)\n",
    "    md_text = to_markdown(paper_pdf)\n",
    "    \n",
    "    return Document(\n",
    "        doc_id=paper_id,\n",
    "        text=md_text,\n",
    "        metadata={\n",
    "            'title': paper.title,\n",
    "            'authors': \", \".join([auth.name for auth in paper.authors[:10]]),\n",
    "            'published': paper.published.strftime('%Y-%m-%d'),\n",
    "            'filepath': pdf_path,\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "paper_id = \"2312.00752\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the documents\n",
    "documents = [\n",
    "    load_arxiv_document(paper_id)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Breakdown the document into nodes\n",
    "parser = MarkdownNodeParser()\n",
    "\n",
    "nodes = parser.get_nodes_from_documents(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the VectorDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a Chroma VectorDB instance\n",
    "db = chromadb.PersistentClient(path=\"./chroma_db\")\n",
    "openai_ef = embedding_functions.OpenAIEmbeddingFunction(\n",
    "                api_key=os.getenv(\"OPENAI_API_KEY\"),\n",
    "                model_name=\"text-embedding-3-small\"\n",
    "            )\n",
    "chroma_collection = db.get_or_create_collection(\"chatxiv\", embedding_function=openai_ef)\n",
    "\n",
    "# Create a Chroma VectorStore\n",
    "vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
    "\n",
    "# Create a Simple Document Store as well\n",
    "docstore = SimpleDocumentStore()\n",
    "\n",
    "# Load the embedding model\n",
    "embed_model = OpenAIEmbedding(model=\"text-embedding-3-small\", embed_batch_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the Embedding Pipeline\n",
    "pipeline = ...\n",
    "\n",
    "# load local cache (if it exists)\n",
    "# pipeline.load(\"./pipeline_chatxiv\")\n",
    "\n",
    "# Run Pipeline\n",
    "\n",
    "# save cache locally\n",
    "pipeline.persist(\"./pipeline_chatxiv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create vector index for retrieval\n",
    "index = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_str = \"Explain mamba\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieved_nodes = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieved_nodes[0].metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Response Synthesis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = {\n",
    "    'role': 'system',\n",
    "    'content': \"\"\"You are a Q&A bot. You are here to answer questions based on the context given.\n",
    "You are prohibited from using prior knowledge and you can only use the context given. If you need \n",
    "more information, please ask the user.\"\"\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_prompt = PromptTemplate(\n",
    "\"\"\"Context information to answer the query is below.\n",
    "---------------------\n",
    "{context_str}\n",
    "---------------------\"\"\")\n",
    "\n",
    "def build_context_prompt(retrieved_nodes):\n",
    "    context_str = \"\\n\\n\".join([r.node.get_content(metadata_mode='all') for r in retrieved_nodes])\n",
    "    return context_prompt.format(\n",
    "        context_str=context_str\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatLLM()\n",
    "session = ChatSession(llm, system_prompt=system_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the context as a user message to the message history (thread)\n",
    "session.thread.append({\n",
    "    'role': 'user',\n",
    "    'content': build_context_prompt(retrieved_nodes)\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = session.chat(\"How does mamba work? Explain like I am 5\")\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieval as a Tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.tools import FunctionTool\n",
    "from openai.types.chat.chat_completion_tool_message_param import ChatCompletionToolMessageParam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def context_retrieval(search_query: str, top_k: int=3) -> Tuple[str, List[NodeWithScore]]:\n",
    "    '''\n",
    "    This function let's you semantically retrieve relevant context chunks from a given document based on a query.\n",
    "\n",
    "    Arguments:\n",
    "        query (str): The query to search for in the document. Based on the original user query, write a good search query\n",
    "                     which is more logically sound to retrieve the relevant information from the document.\n",
    "        top_k (int): The number of top chunks to retrieve from the document. Default is 3. You can increase this number if\n",
    "                     you feel like you need more information. But ideally you should make multiple calls to retrieve different\n",
    "                     topics of information.\n",
    "\n",
    "    Returns:\n",
    "        str: The top retrieved\n",
    "        List[NodeWithScore]: A list of nodes with their scores. Use this to cite the information in the response.\n",
    "    '''\n",
    "    retriever = index.as_retriever(similarity_top_k=top_k)\n",
    "    retrieved_nodes = retriever.retrieve(search_query)\n",
    "    return build_context_prompt(retrieved_nodes), retrieved_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatLLM()\n",
    "session = ChatSession(llm, system_prompt=system_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_retrieval_tool = FunctionTool.from_defaults(fn=context_retrieval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "available_functions = {\n",
    "    \"context_retrieval\": context_retrieval,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conversation_turn(user_query):\n",
    "    # Send the messages and available functions to the model\n",
    "    # We will call our chat method from our session object which is already keeping track of the history\n",
    "    response = session.chat(user_query, \n",
    "                            model='gpt-3.5-turbo', \n",
    "                            temperature=1,\n",
    "                            max_tokens=512,\n",
    "                            tools=[context_retrieval_tool.metadata.to_openai_tool()], tool_choice='auto')\n",
    "    \n",
    "    tool_calls = response.tool_calls\n",
    "\n",
    "    # Check if the model wanted to call a function\n",
    "    if tool_calls:\n",
    "        # Call the function\n",
    "        for tool_call in tool_calls:\n",
    "            function_name = ...\n",
    "            function_to_call = ...\n",
    "            function_args = ...\n",
    "            function_response, retrieved_nodes = function_to_call(**function_args)\n",
    "\n",
    "            print(f'Retrieving with {tool_call}')\n",
    "\n",
    "            # Add the function response to the message thread\n",
    "            session.thread.append(\n",
    "                ChatCompletionToolMessageParam(role='tool', tool_call_id=tool_call.id, name=function_name, content=function_response)\n",
    "            )\n",
    "        \n",
    "        # Get a new response from the model where it can see the function response\n",
    "        post_function_response = session.chat()\n",
    "\n",
    "        return post_function_response\n",
    "    \n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = conversation_turn(\"What are selective state spaces though?\")\n",
    "response.content"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
